[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Programming for everybody session 3",
    "section": "",
    "text": "This course will show how one can treat the Internet as a source of data. We will scrape, parse, and read web data as well as access data using web APIs. We will work with HTML, XML, and JSON data formats in Python."
  },
  {
    "objectID": "code/2_reg_expressions.html",
    "href": "code/2_reg_expressions.html",
    "title": "2  Regular expressions",
    "section": "",
    "text": "In computing a regular expression, also referred to as regex or regexp, provides a concise and flexible means for matching strings of text, such as particular characters, words or patterns of characters.\nA regular expression is written in a formal language that can be interpreted by a regular expression processor.\nIt allows to use “wild card” expressions for matching and parsing strings."
  },
  {
    "objectID": "code/2_reg_expressions.html#the-regular-expression-module",
    "href": "code/2_reg_expressions.html#the-regular-expression-module",
    "title": "2  Regular expressions",
    "section": "2.2 The regular expression module",
    "text": "2.2 The regular expression module\n\nBefore you can use regular expressions in python, you most import the library using import re\nYou can use re.search() to see if a string matches a regular expression, similar to using the find() method for strings\nYou can use re.findall() to extract positions of a string that match your regular expression, similar to using a combination of find() and slicing.\n\n\n#import lib\nimport re\n\nBefore we did:\n\nhand = open('../data/mbox-short.txt')\n\nfor line in hand:\n    line = line.rstrip()\n    if line.find('From: s') >=0 :\n        print(line)\n\nprint('')\n\nFrom: stephen.marquard@uct.ac.za\nFrom: stephen.marquard@uct.ac.za\n\n\n\nBut instead we could use a regular expression:\n\nhand = open('../data/mbox-short.txt')\n\nfor line in hand:\n    line = line.rstrip()\n    if re.search('From: s', line):\n        print(line)\n\nprint('')\n\nFrom: stephen.marquard@uct.ac.za\nFrom: stephen.marquard@uct.ac.za\n\n\n\nUse a regular expression instead of startswith(). Here , we fine tune what is matched by adding special characters to the string, i.e. ^\n\nhand = open('../data/mbox-short.txt')\n\nfor line in hand:\n    line = line.rstrip()\n    if re.search('^From: s', line):\n        print(line)\n\nprint('')\n\nFrom: stephen.marquard@uct.ac.za\nFrom: stephen.marquard@uct.ac.za"
  },
  {
    "objectID": "code/2_reg_expressions.html#wildcard-characters",
    "href": "code/2_reg_expressions.html#wildcard-characters",
    "title": "2  Regular expressions",
    "section": "2.3 Wildcard characters",
    "text": "2.3 Wildcard characters\n\nThe dot character matches any character\nIf you add the asterisk character, the character is “any number of times”, i.e. 0 or more times\n\n\nhand = open('../data/mbox-short.txt')\n\nlst = list()\nfor line in hand:\n    line = line.rstrip()\n    if re.search('^X-.*:', line):\n        if line not in lst:\n            lst.append(line)\n\nfor i in lst[:4]:\n    print(i)\n\nprint('')\n\nX-Sieve: CMU Sieve 2.3\nX-Authentication-Warning: nakamura.uits.iupui.edu: apache set sender to stephen.marquard@uct.ac.za using -f\nX-Content-Type-Outer-Envelope: text/plain; charset=UTF-8\nX-Content-Type-Message-Body: text/plain; charset=UTF-8\n\n\n\nWe can also be more specific and find everything starting with X- then match any non-whitespace character /S one or more times + . This would discard any hits that have any whitespace between the X- and the colon.\n\nhand = open('../data/mbox-short.txt')\n\nlst = list()\nfor line in hand:\n    line = line.rstrip()\n    if re.search('^X-\\S+:', line):\n        if line not in lst:\n            lst.append(line)\n\nfor i in lst[:5]:\n    print(i)\n\nprint('')\n\nX-Sieve: CMU Sieve 2.3\nX-Authentication-Warning: nakamura.uits.iupui.edu: apache set sender to stephen.marquard@uct.ac.za using -f\nX-Content-Type-Outer-Envelope: text/plain; charset=UTF-8\nX-Content-Type-Message-Body: text/plain; charset=UTF-8\nX-DSPAM-Result: Innocent"
  },
  {
    "objectID": "code/2_reg_expressions.html#re.findall",
    "href": "code/2_reg_expressions.html#re.findall",
    "title": "2  Regular expressions",
    "section": "2.4 re.findall()",
    "text": "2.4 re.findall()\n\nre.search() returns a True/False depending on whether the string matches the regular expression\nIf we actually want the matching strings to be extracted, we use `re.findall()``\n\n\nx = 'My 2 favorite numbers are 19 and 420'\ny = re.findall('[0-9]+', x)\nprint(y)\n\n['2', '19', '420']\n\n\nHere: [0-9]+ Looks for any number between 0-9 and the plus indicates that we look for one or more digits.\nNotice: This gives us a list with three strings, not numbers. If there is no match, we get back an empty list:\n\nx = 'My 2 favorite numbers are 19 and 420'\ny = re.findall('[AEIOU]+', x)\nprint(y)\n\n[]"
  },
  {
    "objectID": "code/2_reg_expressions.html#greedy-matching",
    "href": "code/2_reg_expressions.html#greedy-matching",
    "title": "2  Regular expressions",
    "section": "2.5 Greedy matching",
    "text": "2.5 Greedy matching\nThe repeat characters (* and +) push outward in both directions (greedy) to match the largest possible string:\n\nx = 'From: Using the : character'\ny = re.findall('F.+:', x)\nprint(y)\n\n['From: Using the :']\n\n\nWhy in the example we don’t only get From: ?\nBecause we ask for a pattern, where the first character in the match is an F followed by one or more characters and ending with a colon. Then we get the largest possible string, which is not what we actually are looking for."
  },
  {
    "objectID": "code/2_reg_expressions.html#non-greedy-matching",
    "href": "code/2_reg_expressions.html#non-greedy-matching",
    "title": "2  Regular expressions",
    "section": "2.6 Non-greedy matching",
    "text": "2.6 Non-greedy matching\nNot all regular expression repeat codes are greedy!\nIf you add a ? character, the + and * chill out a bit…\n\nx = 'From: Using the : character'\ny = re.findall('F.+?:', x)\nprint(y)\n\n['From:']\n\n\nHere, we ask for a pattern, where the first character in the match is an F followed by one or more characters that are not greedy and ending with a colon."
  },
  {
    "objectID": "code/2_reg_expressions.html#fine-tuning-string-extraction",
    "href": "code/2_reg_expressions.html#fine-tuning-string-extraction",
    "title": "2  Regular expressions",
    "section": "2.7 Fine-tuning string extraction",
    "text": "2.7 Fine-tuning string extraction\nIf we refine the match for re.findaa() and separately determine which proportion of the match is to be extracted by using parentheses:\n\nx = 'From stephen.marquard@uct.ac.za Sat Jan  4 09:14:16 2008'\n\ny = re.findall('\\S+@\\S+', x)\nprint(y)\n\n['stephen.marquard@uct.ac.za']\n\n\nHere, we find everything that matches one or more blank characters followed by an @ sign followed by one or more non-blank characters.\nHere, we need greedy matching to not only be left with a d@u.\nParentheses are not part of the match, but they tells where to start and stop what string to extract:\n\nx = 'From stephen.marquard@uct.ac.za Sat Jan  4 09:14:16 2008'\n\ny = re.findall('^From (\\S+@\\S+)', x)\nprint(y)\n\n['stephen.marquard@uct.ac.za']\n\n\nI.e. above our search statement looks for pattern starting with a From followed by a space, followed by one or more matching our non-blank characters but the From is not printed, only everything between parentheses is.\nI.e. we tell python to start extracting AFTER From."
  },
  {
    "objectID": "code/2_reg_expressions.html#double-split-patterns",
    "href": "code/2_reg_expressions.html#double-split-patterns",
    "title": "2  Regular expressions",
    "section": "2.8 Double split patterns",
    "text": "2.8 Double split patterns\nSometimes we split a line one way and then grep one of the pieces in the line and split that piece again:\n\nx = 'From stephen.marquard@uct.ac.za Sat Jan  4 09:14:16 2008'\n\n#y = re.findall('@(\\S+)', x)\ny = re.findall('@([^ ]*)', x)\nprint(y)\n\n['uct.ac.za']\n\n\nHere, we look for a pattern that start with an @sign followed by a number of non-blank characters.\n\n[^ ] : Match non-blank character, i.e starting with a ^ inside the brackets means NOT –> match everything that is NOT a space\n[^ ]*: Match many non-blank characters\n\nSome fine-tuning:\n\nx = 'From stephen.marquard@uct.ac.za Sat Jan  4 09:14:16 2008'\n\n#y = re.findall('@(\\S+)', x)\ny = re.findall('^From .*@([^ ]*)', x)\nprint(y)\n\n['uct.ac.za']"
  },
  {
    "objectID": "code/2_reg_expressions.html#escape-character",
    "href": "code/2_reg_expressions.html#escape-character",
    "title": "2  Regular expressions",
    "section": "2.9 Escape character",
    "text": "2.9 Escape character\nIf you want a special regular expression character to just behave normally you prefix it with \\\n\nx = 'We just received $10.00 for cookies'\n\ny = re.findall('\\$[0-9.]+', x)\nprint(y)\n\n['$10.00']"
  },
  {
    "objectID": "code/2_reg_expressions.html#using-regex-example",
    "href": "code/2_reg_expressions.html#using-regex-example",
    "title": "2  Regular expressions",
    "section": "2.10 Using regex example",
    "text": "2.10 Using regex example\n\nimport re\n\nhand = open('../data/mbox-short.txt')\nnumlist = list()\n\nfor line in hand:\n    line = line.rstrip()\n    #extract what we are interest in\n    stuff = re.findall('^X-DSPAM-Confidence: ([0-9.]+)', line)\n    \n    #deal with IndexError: list index out of range\n    #since we know stuff should be a list with one element in there\n    if len(stuff) != 1:\n        continue\n    \n    #extract pattern and convert to float\n    num = float(stuff[0])\n    \n    #add all numbers to lists\n    numlist.append(num)\n\nprint('Maximum: ', max(numlist))\n\nMaximum:  0.9907"
  },
  {
    "objectID": "code/2_reg_expressions.html#exercise",
    "href": "code/2_reg_expressions.html#exercise",
    "title": "2  Regular expressions",
    "section": "2.11 Exercise",
    "text": "2.11 Exercise\nIn this assignment you will read through and parse a file with text and numbers. You will extract all the numbers in the file and compute the sum of the numbers.\nThe basic outline of this problem is to read the file, look for integers using the re.findall(), looking for a regular expression of ‘[0-9]+’ and then converting the extracted strings to integers and summing up the integers.\nCode found in code/count_numbers.py\n\nhandle = open('../data/regex_sum_1701457.txt')\n\nimport re\n\nnumlst = list()\ncounter = 0\n\nfor line in handle:\n    line = line.rstrip()\n\n    #extract list of numbers\n    num = re.findall('[0-9]+', line)\n    if len(num) == 0:\n        continue\n\n    #add numbers to list\n    numlst = numlst + num\n\n#convert to numbers\nfor i in range(0, len(numlst)):\n    numlst[i] = int(numlst[i])\n\nprint(sum(numlst))\n\n383660"
  },
  {
    "objectID": "code/3_networks_and_sockets.html",
    "href": "code/3_networks_and_sockets.html",
    "title": "3  Networks and sockets",
    "section": "",
    "text": "Build on top of the Internet Protocol (IP); i.e. a protocol, or set of rules, for routing and addressing packets of data so that they can travel across networks and arrive at the correct destination\nAssumes IP might lose some data: stores and re-transmits data if it seems to be lost\nHandles “flow control” using a transmit window\nProvides a nice reliable pipe"
  },
  {
    "objectID": "code/3_networks_and_sockets.html#tcp-connectionssockets",
    "href": "code/3_networks_and_sockets.html#tcp-connectionssockets",
    "title": "3  Networks and sockets",
    "section": "3.2 TCP connections/sockets",
    "text": "3.2 TCP connections/sockets\nA protocol, or set of rules, for routing and addressing packets of data so that they can travel across networks and arrive at the correct destination\nIn computer networking, an Internet socket or network socket is an endpoint of a bidirectional inter-process communication flow across an Internet Protocol-based computer network, such as the Internet.\nProcess <– Internet –> Process"
  },
  {
    "objectID": "code/3_networks_and_sockets.html#tcp-port-numbers",
    "href": "code/3_networks_and_sockets.html#tcp-port-numbers",
    "title": "3  Networks and sockets",
    "section": "3.3 TCP Port Numbers",
    "text": "3.3 TCP Port Numbers\n\nA port is an application-specific or process-specific software communications endpoint\nIt allows multiple networked applications to co-exist on the same server\nThere is a list of well-known TCP port numbers, i.e. port 80 is the web port that is connected to a web server"
  },
  {
    "objectID": "code/3_networks_and_sockets.html#sockets-in-python",
    "href": "code/3_networks_and_sockets.html#sockets-in-python",
    "title": "3  Networks and sockets",
    "section": "3.4 Sockets in Python",
    "text": "3.4 Sockets in Python\nPython has build-in support for TCP sockets:\n\nimport socket\n\n\n#create a socket to create an endpoint on our computer\n#its not yet connected\nmysocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\n#connect by enter the host (data.pre4.e.org) and the host (80)\n#this is not sending data, its like dialing a phone\nmysocket.connect(('data.pr4e.org', 80))\n\n#close connection\nmysocket.shutdown(socket.SHUT_RDWR)\nmysocket.close()"
  },
  {
    "objectID": "code/3_networks_and_sockets.html#application-protocol",
    "href": "code/3_networks_and_sockets.html#application-protocol",
    "title": "3  Networks and sockets",
    "section": "3.5 Application protocol",
    "text": "3.5 Application protocol\n\nSince the TCP (and Python) gives us a reliable socket, what do we want to do with the socket? What problem do we want to solve?\nApplication Protocols: There are different rules how to talk to for example a mail compared to a web server."
  },
  {
    "objectID": "code/3_networks_and_sockets.html#http-hypertext-transfer-protocol",
    "href": "code/3_networks_and_sockets.html#http-hypertext-transfer-protocol",
    "title": "3  Networks and sockets",
    "section": "3.6 HTTP: Hypertext Transfer Protocol",
    "text": "3.6 HTTP: Hypertext Transfer Protocol\n\nThe dominant Application Layer Protocol on the Internet\nInvented for the web to retrieve HTML, Images, Documents, …\nExtended to be data in addition to documents: RSS, Web Services, etc.\nThe basic concept is to make a connection, request a document, retrieve the document, close the connection\nHTTP: The HyperText Transfer Protocol is a set of rules that allow browsers to retrieve web documents from servers over the internet\nA protocol is a set of rules that all parties follow so we can predict each others behavior and not bump into each other"
  },
  {
    "objectID": "code/3_networks_and_sockets.html#getting-data-from-the-server",
    "href": "code/3_networks_and_sockets.html#getting-data-from-the-server",
    "title": "3  Networks and sockets",
    "section": "3.7 Getting data from the server",
    "text": "3.7 Getting data from the server\nEach time the user clicks on an anchor tag with an href= value to switch to a new page, the browser makes a connection to the webserver and issues a get request to get the content of the page at the specified URL.\nThe server returns the HTML document to the browser which formats and displays the document to the user."
  },
  {
    "objectID": "code/3_networks_and_sockets.html#internet-standards",
    "href": "code/3_networks_and_sockets.html#internet-standards",
    "title": "3  Networks and sockets",
    "section": "3.8 Internet standards",
    "text": "3.8 Internet standards\n\nThe standards for all of the Internet protocols (inner workings) are developed by an organization, the Internet Engineering Task Force (IETF)\nwww.ietf.org\nStandards are called RFCs, Request for comments"
  },
  {
    "objectID": "code/3_networks_and_sockets.html#an-http-request-in-python",
    "href": "code/3_networks_and_sockets.html#an-http-request-in-python",
    "title": "3  Networks and sockets",
    "section": "3.9 An HTTP Request in python",
    "text": "3.9 An HTTP Request in python\n\nimport socket \n\n#create a socket to create an endpoint on our computer\n#its not yet connected\nmysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\n#connect by enter the host (data.pre4.e.org) and the host (80)\n#this is not sending data, its like dialing a phone\nmysock.connect(('data.pr4e.org', 80))\n\n#make a string with an request\ncmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n\n#send request\nmysock.send(cmd)\n\n#receive (recv) up to 512 characters\n#if we don't get data, we break out of the loop\n#if we get data,we decode it\nwhile True:\n    data = mysock.recv(512)\n    if (len(data) < 1):\n        break\n    print(data.decode())\n\n#close connection\nmysock.close()\n\nHTTP/1.1 200 OK\nDate: Sat, 10 Dec 2022 11:15:18 GMT\nServer: Apache/2.4.18 (Ubuntu)\nLast-Modified: Sat, 13 May 2017 11:22:22 GMT\nETag: \"a7-54f6609245537\"\nAccept-Ranges: bytes\nContent-Length: 167\nCache-Control: max-age=0, no-cache, no-store, must-revalidate\nPragma: no-cache\nExpires: Wed, 11 Jan 1984 05:00:00 GMT\nConnection: close\nContent-Type: text/plain\n\nBut soft what light through yonder window breaks\nIt is the east and Juliet is the sun\nArise fair sun and kill the envious moon\nWho is already s\nick and pale with grief\n\n\n\nAbove we get:\n\nThe HTTP header including the metadata\nHTTP body"
  },
  {
    "objectID": "code/3_networks_and_sockets.html#if-you-want-to-know-more",
    "href": "code/3_networks_and_sockets.html#if-you-want-to-know-more",
    "title": "3  Networks and sockets",
    "section": "3.10 If you want to know more",
    "text": "3.10 If you want to know more\nThis chapter covers networking at a very high level. If you want to learn more, there is both a free book and a Coursera course that I would recommend:\nIntroduction to Networking (free textbook)\nInternet History, Technology, and Security (Coursera Course)\nNeither of these is essential for this course or the Python Specialization as we quickly move from how the network works to how to write Python code using the urllib library - which makes the very complex Internet protocols exceedingly simple."
  },
  {
    "objectID": "code/3_networks_and_sockets.html#assignment",
    "href": "code/3_networks_and_sockets.html#assignment",
    "title": "3  Networks and sockets",
    "section": "3.11 Assignment",
    "text": "3.11 Assignment\nYou are to retrieve the following document using the HTTP protocol in a way that you can examine the HTTP Response headers.\n\nhttp://data.pr4e.org/intro-short.txt\n\nYou might retrieve this web page and look at the response headers: Modify the socket1.py program to retrieve the above URL and print out the headers and data. Make sure to change the code to retrieve the above URL - the values are different for each URL."
  },
  {
    "objectID": "code/4_surf_the_web.html",
    "href": "code/4_surf_the_web.html",
    "title": "4  Programs that surf the web",
    "section": "",
    "text": "ASCII, American Standard Code for Information Interchange, commonly used mapping of numbers to letters in programming\nI.e. H == 72\n\n\n\n\nEach character is represented by a number between 0 and 256 stored in 8 bits of memory\nWe only have a set amount of numbers that can represent characters, so in the earlier days of programming only upper case characters were represented and lower case “did not exist”\nWe refer to “8 bits of memories” as a byte\nIn the 1960s and 1970s we could “put one character in a byte”\nThe ord() function tells us the numeric value of a simple ASCII character\n\n\nprint(ord('G'))\n\n71\n\n\n\nprint(ord('e'))\n\n101\n\n\nWith the 2 examples above, we can see that H has a lower ordinal number than e, so H < e. All uppercase are less than lower case letters.\n\nprint(ord('\\n'))\n\n10\n\n\n\n\n\nUnicode is a universal code for hundreds and millions of different characters.\n\n\n\n\nTo represent the wide range of characters a computer must handle, we represent characters with more than one byte:\nUTF-16: Fixed length, two bytes\nUTF-32: Fixed length, four bytes. Here, each character uses 4 bytes, so this is less efficient in terms of how much space we need to for example UTF-16\nUTF-8: 1-4 bytes:\n\nMost flexible in terms of space requirements\nUpwards compatible with ASCII\nAutomatic detection between ASCII and UTF-8\nUTF-8 is recommended practice for encoding data to be exchanged between systems\n\n\n\n\n\n\nIn Py2 we have two kinds of strings: a “normal” str and a unicode str\nIn Py3 all strings are Unicode strings, making all strings the same regardless whether we have Japanese or Latin characters. But in Py3 we have byte strings, i.e. a sequence of bytes that is not human readable and from which we do not know the encoding\n\n\n\n\n\nIn Py3 all strings internally are unicode\nWorking with string variables in python programs and reading data usually “just works”\nWhen we talk to a network resource using sockets or talk to a datase we have to encode and decode data (usually to UTF-8)\n\n\n\n\n\nWhen we talk to an external resource like a network socket we send bytes, so we need to encode the Py3 strings into a given character encoding\nWhen we read data from an external resource, we must decode it based on the character set so it is properly represented in Py3 as a string:\n\n\nimport socket \n\nmysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\nmysock.connect(('data.pr4e.org', 80))\n\n#before sending the data we turn them into bytes\ncmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n\n#send request\nmysock.send(cmd)\n\nwhile True:\n    #data we receive is in bytes\n    data = mysock.recv(512)\n    if (len(data) < 1) :\n        break\n    #by default decode() assumes UFT8 or ASCII\n    mystring = data.decide()\n    print(mystring)"
  },
  {
    "objectID": "code/4_surf_the_web.html#retrieving-webpages",
    "href": "code/4_surf_the_web.html#retrieving-webpages",
    "title": "4  Programs that surf the web",
    "section": "4.2 Retrieving webpages",
    "text": "4.2 Retrieving webpages\nInstead of sockets, we can even go simpler using urllib, a library that does all the socket work for us and makes web pages look like a file:\n\nimport urllib.request, urllib.parse, urllib.error\n\n#get a handle, that does not contain the data but allows us to access the data\nfhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n\n#iterate through the data using a for loop\nfor line in fhand:\n    print(line.decode().strip())\n\nprint('')\n\nBut soft what light through yonder window breaks\nIt is the east and Juliet is the sun\nArise fair sun and kill the envious moon\nWho is already sick and pale with grief\n\n\n\nIn contrast to when we used the socket library we only get the body and not the header with the metadata. But urllib keeps and remembers them, so we could ask for this information.\nUrllib kind of handles a webpage like a file and we can use this writing whatever loop we want, i.e. we can count words like this:\n\nimport urllib.request, urllib.parse, urllib.error\n\n#get a handle, that does not contain the data but allows us to access the data\nfhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n\ncounts = dict()\n\n#iterate through the data using a for loop\nfor line in fhand:\n    words = line.decode().split()\n    for word in words:\n        counts[word] = counts.get(word, 0) + 1\n\nprint(counts)\n\n{'But': 1, 'soft': 1, 'what': 1, 'light': 1, 'through': 1, 'yonder': 1, 'window': 1, 'breaks': 1, 'It': 1, 'is': 3, 'the': 3, 'east': 1, 'and': 3, 'Juliet': 1, 'sun': 2, 'Arise': 1, 'fair': 1, 'kill': 1, 'envious': 1, 'moon': 1, 'Who': 1, 'already': 1, 'sick': 1, 'pale': 1, 'with': 1, 'grief': 1}\n\n\nThe only difference to handling a “normal” file is that we need to remember to decode the data after we retrieved it.\nWe can also use this to retrieve html sites:\n\nimport urllib.request, urllib.parse, urllib.error\n\n#get a handle, that does not contain the data but allows us to access the data\nfhand = urllib.request.urlopen('http://dr-chuck.com/page1.htm')\n\n#iterate through the data using a for loop\nfor line in fhand:\n    print(line.decode().strip())\n\nprint('')\n\n<h1>The First Page</h1>\n<p>\nIf you like, you can switch to the\n<a href=\"http://www.dr-chuck.com/page2.htm\">\nSecond Page</a>.\n</p>\n\n\n\n\n4.2.1 Following links\nIf we want to identify and print all links on a web page,\ni.e. the ` we could do it as follows:\n\nimport urllib.request, urllib.parse, urllib.error\nimport re\n\n#get a handle, that does not contain the data but allows us to access the data\nfhand = urllib.request.urlopen('http://dr-chuck.com/page1.htm')\n\nlink_l = list()\n\n#iterate through the data using a for loop\nfor line in fhand:\n    line = line.decode().strip()\n    link = re.findall('href=\"(http:.+)\"', line)\n    if len(link) == 0:\n        continue\n    link_l = link_l + link\n\nprint(link_l)\n\n['http://www.dr-chuck.com/page2.htm']"
  },
  {
    "objectID": "code/4_surf_the_web.html#what-is-web-scraping",
    "href": "code/4_surf_the_web.html#what-is-web-scraping",
    "title": "4  Programs that surf the web",
    "section": "4.3 What is web scraping?",
    "text": "4.3 What is web scraping?\n\nWhen a program or a script pretends to be a browser and retrieves web pages, looks at those web pages, extracts information, and then looks at more web pages\nSearch engines scrape web pages, we call this “spidering the web” or “web crawling”"
  },
  {
    "objectID": "code/4_surf_the_web.html#why-scrape",
    "href": "code/4_surf_the_web.html#why-scrape",
    "title": "4  Programs that surf the web",
    "section": "4.4 Why scrape?",
    "text": "4.4 Why scrape?\n\nTo pull out data, particularly social data, who links who?\nGet your own data back out of some system that has no export capability\nMonitor a site for new information\nSpider the web to make a database or a search engine\nKeep in mind, you have to be allowed to access the data\nThere is some controversy about scraping and some sites have some strict rules around it\nRepublishing copyrighted information is not allowed\nViolating terms of service is not allowed"
  },
  {
    "objectID": "code/4_surf_the_web.html#beautiful-soup-for-parsing-html",
    "href": "code/4_surf_the_web.html#beautiful-soup-for-parsing-html",
    "title": "4  Programs that surf the web",
    "section": "4.5 Beautiful Soup for parsing HTML",
    "text": "4.5 Beautiful Soup for parsing HTML\n\nYou could do HTML searches the hard way (since HTML is not allways super consistent)\nOr use a free software library called BeautifulSoup\nTo run this software, you have to first install it\n\nCode is found in code/urllinks.py\n\nimport urllib.request, urllib.parse, urllib.error\nfrom bs4 import BeautifulSoup\n\nurl = 'http://www.dr-chuck.com/page1.htm'\nhtml = urllib.request.urlopen(url).read()\n\n#beautify the html object\nsoup = BeautifulSoup(html, 'html.parser')\n\n#retrieve all of the anchor tags\n#this code below returns a list\ntags = soup('a')\n\n#extract the href key or nothing\nfor tag in tags:\n    print(tag.get('href', None))\n\nprint('')\n\nhttp://www.dr-chuck.com/page2.htm\n\n\n\nIf we have pages that give us SSL certificate errors, we can adjust the code like this:\n\nimport urllib.request, urllib.parse, urllib.error\nfrom bs4 import BeautifulSoup\nimport ssl\n\n# Ignore SSL certificate errors\nctx = ssl.create_default_context()\nctx.check_hostname = False\nctx.verify_mode = ssl.CERT_NONE\n\n#beautify the html object\nurl = 'http://www.dr-chuck.com/page1.htm'\nhtml = urllib.request.urlopen(url, context=ctx).read()\nsoup = BeautifulSoup(html, 'html.parser')\n\n#retrieve all of the anchor tags\n#this code below returns a list\ntags = soup('a')\n\n#extract the href key or nothing\nfor tag in tags:\n    print(tag.get('href', None))\n\nprint('')\n\nhttp://www.dr-chuck.com/page2.htm"
  },
  {
    "objectID": "code/4_surf_the_web.html#exercise",
    "href": "code/4_surf_the_web.html#exercise",
    "title": "4  Programs that surf the web",
    "section": "4.6 Exercise",
    "text": "4.6 Exercise\nThe program will use urllib to read the HTML from the data files below, and parse the data, extracting numbers and compute the sum of the numbers in the file.\nWe provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n\nSample data: http://py4e-data.dr-chuck.net/comments_42.html (Sum=2553)\nActual data: http://py4e-data.dr-chuck.net/comments_1701459.html (Sum ends with 85)\n\nThe file is a table of names and comment counts. You can ignore most of the data in the file except for lines like the following:\n<tr><td>Modu</td><td><span class=\"comments\">90</span></td></tr>\n<tr><td>Kenzie</td><td><span class=\"comments\">88</span></td></tr>\n<tr><td>Hubert</td><td><span class=\"comments\">87</span></td></tr>\nYou are to find all the  tags in the file and pull out the numbers from the tag and sum the numbers.\nCode is found in code/exercise.py\n\nimport urllib.request, urllib.parse, urllib.error\nfrom bs4 import BeautifulSoup\nimport re\n\n#url = \nurl = 'http://py4e-data.dr-chuck.net/comments_1701459.html'\nhtml = urllib.request.urlopen(url).read()\n\n#beautify\nsoup = BeautifulSoup(html, 'html.parser')\n\n#retrieve span tags\ntags = soup('span')\n\n#pull out nrs\nnumlist = list()\n\nfor tag in tags:\n    tag_string = int(tag.contents[0])\n    numlist.append(tag_string)\n\nprint('Count', len(numlist))\nprint('Sum', sum(numlist))\n\nCount 50\nSum 2685"
  },
  {
    "objectID": "code/4_surf_the_web.html#exercise-2",
    "href": "code/4_surf_the_web.html#exercise-2",
    "title": "4  Programs that surf the web",
    "section": "4.7 Exercise 2",
    "text": "4.7 Exercise 2\nThe program will use urllib to read the HTML from the data files below, extract the href= vaues from the anchor tags, scan for a tag that is in a particular position relative to the first name in the list, follow that link and repeat the process a number of times and report the last name you find.\nWe provide two files for this assignment. One is a sample file where we give you the name for your testing and the other is the actual data you need to process for the assignment\nSample problem: Start at http://py4e-data.dr-chuck.net/known_by_Fikret.html Find the link at position 3 (the first name is 1). Follow that link. Repeat this process 4 times. The answer is the last name that you retrieve. Sequence of names: Fikret Montgomery Mhairade Butchi Anayah Last name in sequence: Anayah\nActual problem: Start at: http://py4e-data.dr-chuck.net/known_by_Josh.html Find the link at position 18 (the first name is 1). Follow that link. Repeat this process 7 times. The answer is the last name that you retrieve. Hint: The first character of the name of the last page that you will load is: G\nStrategy The web pages tweak the height between the links and hide the page after a few seconds to make it difficult for you to do the assignment without writing a Python program. But frankly with a little effort and patience you can overcome these attempts to make it a little harder to complete the assignment without writing a Python program. But that is not the point. The point is to write a clever Python program to solve the program.\nSample execution Here is a sample execution of a solution:\n$ python3 solution.py\nEnter URL: http://py4e-data.dr-chuck.net/known_by_Fikret.html\nEnter count: 4\nEnter position: 3\nRetrieving: http://py4e-data.dr-chuck.net/known_by_Fikret.html\nRetrieving: http://py4e-data.dr-chuck.net/known_by_Montgomery.html\nRetrieving: http://py4e-data.dr-chuck.net/known_by_Mhairade.html\nRetrieving: http://py4e-data.dr-chuck.net/known_by_Butchi.html\nRetrieving: http://py4e-data.dr-chuck.net/known_by_Anayah.html"
  },
  {
    "objectID": "code/5_web_services_and_xml.html",
    "href": "code/5_web_services_and_xml.html",
    "title": "5  Web services and XML",
    "section": "",
    "text": "With the HTTP request/response well understood and supported, there was a natural move towards exchanging data between programs and these protocols\nWe needed to come up with up with an agreed way to represent data going between applications and across networks\nTwo commonly used formats are XML and JSON"
  },
  {
    "objectID": "code/5_web_services_and_xml.html#extensible-markup-language-xml",
    "href": "code/5_web_services_and_xml.html#extensible-markup-language-xml",
    "title": "5  Web services and XML",
    "section": "5.2 eXtensible Markup Language (XML)",
    "text": "5.2 eXtensible Markup Language (XML)\n\nPrimary purpose is to help information systems share structured data\nIt started as a simplified subset of the Standard Generalized Markup Language (SGML) and is designed to relatively human-legible\nConsists of XML elements (or nodes) that have start and end tags in between smaller greater signs, i.e. \nContains simple and complex elements\n\n\n5.2.1 XML terminology\n\nTags indicate the beginning and the ending of elements\nAttributes are keyword/value pairs on the opening tag of XML\nSerialize and De-serialize convert data in one program into a common format that can be stored and/or transmitted between systems in a programming language-independent manner\n\n\n\n5.2.2 The XML tree structure\n\nXML documents form a tree structure that starts at “the root” and branches to “the leaves”.\nXML documents are formed as element trees.\nAn XML tree starts at a root element and branches from the root to child elements.\nAll elements can have sub elements (child elements):\n\n<root>\n  <child>\n    <subchild>.....</subchild>\n  </child>\n</root>\n\nThe terms parent, child, and sibling are used to describe the relationships between elements.\nParents have children. Children have parents. Siblings are children on the same level (brothers and sisters).\n\n\n\n\n5.2.3 XML basics\n\nThe difference to HTML is that we can create our own tags\nStart Tag, i.e. \nEnd tag, i.e. \nText content, i.e. Chuck\n\nTogether we can make a simple element like this:\n<name>Chuck</name>\n\nAttribute, i.e. type “intl”\n\n<phone type=\"intl\">\n+1 734 303 4456\n</phone>\n\nSelf closing tag incl. an attribute i.e. <email hide=\"yes\" />\n\n\n\n5.2.4 White space\nLine ends don’t really matter in XML,\nWhite space is generally discarded on text elements.\nWe indent only to be readable.\n\n\n5.2.5 XML schema\n\nDescribing a “contract” as to what is acceptable in XML\nDescription of the legal format of an XML document\nExpressed in terms of constraints on the structure and content of documents\nOften used to specify a “contract” between systems: I.e. my system will only accept XML that confirms to this particular schema\nIf a particular piece of XML meets the specification of the schema, then it is said to validate\n\n\n\n5.2.6 Many XML schema languages\n\nDocument type definition (DTD)\nStandard Generalized Markup language (ISO 8879:1986 SGML)\nXML schema for W3C (XSD), the most likely one we encounter\n\n\n\n5.2.7 ISO 8601 Date/Time format\n2002-05-30T09:30:10Z\nAbove we have:\n\nYear-month-date\nTime of the day\nTime zone (i.e.T…Z), which typically is specified in UTC/GMT rather than a local time zone"
  },
  {
    "objectID": "code/5_web_services_and_xml.html#parsing-xml-in-python",
    "href": "code/5_web_services_and_xml.html#parsing-xml-in-python",
    "title": "5  Web services and XML",
    "section": "5.3 Parsing XML in python",
    "text": "5.3 Parsing XML in python\n\n#Use ET as an alias so we don't have to write the whole thing\nimport xml.etree.ElementTree as ET\n\n#create some xml\n#tripple quoted string = multiline string in python\ndata = '''<person>\n    <name>Chuck</name>\n    <phone type=\"intl\">+1 734 303 4456</phone>\n    <email hide=\"yes\"/>\n    </person>'''\n\n#take the string data and give us back a nice tree\n#this would fail if you have syntax errors\ntree = ET.fromstring(data)\n\n#within that xml data find some tags, i.e. name,\n#and extract the text\nprint('Name:', tree.find('name').text)\n\n#within that xml data find some tags, i.e. email\n#and get the attribute hide using the get method\nprint('Attr:', tree.find('email').get('hide'))\n\nName: Chuck\nAttr: yes\n\n\nThis works a bit differently if we have multiple child tags, i.e. below we want to extract information for each of these user tags:\n\nimport xml.etree.ElementTree as ET\n\ninput = '''\n<stuff>\n  <users>\n    <user x=\"2\">\n      <id>001</id>\n      <name>Chuck</name>\n    </user>\n    <user x=\"7\">\n      <id>009</id>\n      <name>Brent</name>\n    </user>\n  </users>\n</stuff>'''\n\n#get tree\nstuff = ET.fromstring(input)\n\n#search for all of the user tags below users\n#this gives us a list of tags (i.e. a list of subtrees)\nlst = stuff.findall('users/user')\nprint('User count:', len(lst))\nprint('')\n\n#loop through the list to get the info we want\n#i.e. find the tag name and get the child text\nfor item in lst:\n    print('Name:', item.find('name').text)\n    print('ID:', item.find('id').text)\n    print('Attribute:', item.get('x'))\n\nprint('')\n\nUser count: 2\n\nName: Chuck\nID: 001\nAttribute: 2\nName: Brent\nID: 009\nAttribute: 7"
  },
  {
    "objectID": "code/5_web_services_and_xml.html#extracting-data-from-xml",
    "href": "code/5_web_services_and_xml.html#extracting-data-from-xml",
    "title": "5  Web services and XML",
    "section": "5.4 Extracting Data from XML",
    "text": "5.4 Extracting Data from XML\nIn this assignment you will write a Python program somewhat similar to https://py4e.com/code3/geoxml.py.\nThe program will prompt for a URL, read the XML data from that URL using urllib and then parse and extract the comment counts from the XML data, compute the sum of the numbers in the file and enter the sum.\nWe provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n\nSample data: http://py4e-data.dr-chuck.net/comments_42.xml (Sum=2553)\nActual data: http://py4e-data.dr-chuck.net/comments_1701461.xml (Sum ends with 75)\n\nYou do not need to save these files to your folder since your program will read the data directly from the URL.\nThe data consists of a number of names and comment counts in XML as follows:\n<comment>\n  <name>Matthias</name>\n  <count>97</count>\n</comment>\nYou are to look through all the  tags and find the  values sum the numbers.\nTo make the code a little simpler, you can use an XPath selector string to look through the entire tree of XML for any tag named ‘count’ with the following line of code:\ncounts = tree.findall('.//count')\nCode can be found in code/extract_xml.py\n\nimport urllib.request, urllib.parse, urllib.error\nimport xml.etree.ElementTree as ET\nimport ssl\n\n# Ignore SSL certificate errors\nctx = ssl.create_default_context()\nctx.check_hostname = False\nctx.verify_mode = ssl.CERT_NONE\n\n#read in data\nurl = 'http://py4e-data.dr-chuck.net/comments_1701461.xml'\ninput = urllib.request.urlopen(url, context=ctx)\ndata = input.read()\n\nstuff = ET.fromstring(data)\n\n#find comments\nlst = stuff.findall('comments/comment')\n#print('Comment count:', len(lst))\n\n#extract numbers:\nnum_list = list()\n\nfor item in lst:\n    num = int(item.find('count').text)\n    num_list.append(num)\n\nprint('Sum:', sum(num_list))\n\nSum: 2375"
  },
  {
    "objectID": "code/6_JSON_and_REST.html",
    "href": "code/6_JSON_and_REST.html",
    "title": "6  JSON and REST",
    "section": "",
    "text": "Object literal notation in Javascript\nJSON represents data as nested “lists” and “dictionaries”\nIts not as rich as XML but more simpler, making it easier to use\n\n\nimport json\n\n#curly braces created an object in JSON, which are similar to PY dictionaries\ndata = '''{\n    \"name\": \"Chuck\",\n    \"phone\": {\n        \"type\": \"intl\",\n        \"number\": \"222\"\n    },\n    \"email\": {\n        \"hide\": \"yes\"\n    }\n}'''\n\n#load from string and give us an object back\n#this gives us back a py dictionary\ninfo = json.loads(data)\nprint(info)\n\n{'name': 'Chuck', 'phone': {'type': 'intl', 'number': '222'}, 'email': {'hide': 'yes'}}\n\n\n\n#extract the value using the name key\nprint('Name:', info['name'])\nprint('Hide:', info['email']['hide'])\n\nName: Chuck\nHide: yes\n\n\nAn example of a list:\n\nimport json\n\n#generate a list of two dictionaries using the square bracket followed by curly ones\ninput = '''[\n    {\"id\":\"001\",\n    \"x\": \"2\",\n    \"name\": \"Chuck\"\n    },\n    {\"id\": \"009\",\n    \"x\" : \"7\",\n    \"name\": \"Jack\"\n    }\n]'''\n\ninfo = json.loads(input)\nprint(info)\n\n[{'id': '001', 'x': '2', 'name': 'Chuck'}, {'id': '009', 'x': '7', 'name': 'Jack'}]\n\n\n\n#how many users are in this file\nprint('User counts:', len(info))\n\n#exract user data\nfor item in info:\n    print('Name:', item['name'])\n    print('Id', item['id'])\n\nprint('')\n\nUser counts: 2\nName: Chuck\nId 001\nName: Jack\nId 009"
  },
  {
    "objectID": "code/6_JSON_and_REST.html#service-oriented-approach",
    "href": "code/6_JSON_and_REST.html#service-oriented-approach",
    "title": "6  JSON and REST",
    "section": "6.2 Service oriented approach",
    "text": "6.2 Service oriented approach\n\nMost non-trivial web applications use services\nThey use services from other applications, i.e.:\n\nCredit card charge\nHotel reservation systems\n\nServices publish the “rules” applications must follow to make use of a service (API, application programming interface)\nInitially two systems cooperate and split the problem\nAs the data/service becomes useful, multiple applications want to use the information/application\n\nAnother working example is in code/geojson.py\nBelow, we work with the GoogleMaps API. We have to pass the address using URL encoding:\nhttp://py4e-data.dr-chuck.net/json?address=Den+Helder&key=42\nHere:\n\nSpaces are +\n%2C would be a comma\n\nIf the URL is correct this gives us a JSON back.\nWe will talk API keys a bit later.\n\nimport urllib.request, urllib.parse, urllib.error\nimport json\nimport ssl\n\napi_key = False\n# If you have a Google Places API key, enter it here\n# api_key = 'AIzaSy___IDByT70'\n# https://developers.google.com/maps/documentation/geocoding/intro\n\n#An API's service URL is the URL used to consume the API using the configured authentication method\nif api_key is False:\n    api_key = 42\n    serviceurl = 'http://py4e-data.dr-chuck.net/json?'\nelse :\n    serviceurl = 'https://maps.googleapis.com/maps/api/geocode/json?'\n\n# Ignore SSL certificate errors\nctx = ssl.create_default_context()\nctx.check_hostname = False\nctx.verify_mode = ssl.CERT_NONE\n\naddress = 'Den Helder'\n\nparms = dict()\nparms['address'] = address\nif api_key is not False: parms['key'] = api_key\n\n#add the location to the service URL in the right way\nurl = serviceurl + urllib.parse.urlencode(parms)\nprint('Retrieving', url)\n\n#get the data handle\nuh = urllib.request.urlopen(url, context=ctx)\ndata = uh.read().decode()\nprint('Retrieved', len(data), 'characters')\n\ntry:\n    js = json.loads(data)\nexcept:\n    js = None\n\n#ensure that we got good data\n#quit if we got nothing, or no status key\n#in r this gives an error but runs fine in the executable code\n#if not js or 'status' not in js or js['status'] != 'OK':\n#    print(data)\n#    continue\n\n#look at data in a readable format\n#print(json.dumps(js, indent=4))\n\n#walk down the tree to get the data\nlat = js['results'][0]['geometry']['location']['lat']\nlng = js['results'][0]['geometry']['location']['lng']\nprint('lat', lat, 'lng', lng)\n\nlocation = js['results'][0]['formatted_address']\nprint(location)\n\nRetrieving http://py4e-data.dr-chuck.net/json?address=Den+Helder&key=42\n\n\nRetrieved 1735 characters\nlat 52.95628079999999 lng 4.7607972\nDen Helder, Netherlands"
  },
  {
    "objectID": "code/6_JSON_and_REST.html#api-security-and-rate-limiting",
    "href": "code/6_JSON_and_REST.html#api-security-and-rate-limiting",
    "title": "6  JSON and REST",
    "section": "6.3 API security and rate limiting",
    "text": "6.3 API security and rate limiting\n\nThe compute resources to run these APIs are not free\nThe data provided by these APIs is usually valuable\nThe data providers might limit the number of requests per day, demand an API key or even charge for usage\nSome APIs, i.e. twitter, require you to be authorized\nThey might change the rules as things progress\n\nAn example is found in code/twitter2.py\n\nimport urllib.request, urllib.parse, urllib.error\n#notice, this only works if twurl.py and dependency py files are in the same dir\nimport twurl\nimport json\nimport ssl\n\n# https://apps.twitter.com/\n# Create App and get the four strings, put them in hidden.py\n\n#read in twitter service url\nTWITTER_URL = 'https://api.twitter.com/1.1/friends/list.json'\n\n# Ignore SSL certificate errors\nctx = ssl.create_default_context()\nctx.check_hostname = False\nctx.verify_mode = ssl.CERT_NONE\n\nwhile True:\n    print('')\n    acct = input('Enter Twitter Account:')\n    if (len(acct) < 1): break\n    url = twurl.augment(TWITTER_URL,\n                        {'screen_name': acct, 'count': '5'})\n    print('Retrieving', url)\n    \n    connection = urllib.request.urlopen(url, context=ctx)\n    data = connection.read().decode()\n\n    js = json.loads(data)\n    print(json.dumps(js, indent=2))\n\n    #urllib eats the headers, but we can retrieve them like this:\n    #this line shows us how many requests we still have\n    headers = dict(connection.getheaders())\n    print('Remaining', headers['x-rate-limit-remaining'])\n\n    for u in js['users']:\n        print(u['screen_name'])\n        if 'status' not in u:\n            print('   * No status found')\n            continue\n        s = u['status']['text']\n        print('  ', s[:50])\n\nprint('')"
  },
  {
    "objectID": "code/6_JSON_and_REST.html#exercise-1-extract-data-from-json",
    "href": "code/6_JSON_and_REST.html#exercise-1-extract-data-from-json",
    "title": "6  JSON and REST",
    "section": "6.4 Exercise 1: Extract data from JSON",
    "text": "6.4 Exercise 1: Extract data from JSON\nThe program will prompt for a URL, read the JSON data from that URL using urllib and then parse and extract the comment counts from the JSON data, compute the sum of the numbers in the file and enter the sum below:\nWe provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n\nSample data: http://py4e-data.dr-chuck.net/comments_42.json (Sum=2553)\nActual data: http://py4e-data.dr-chuck.net/comments_1701462.json (Sum ends with 0)\n\nYou do not need to save these files to your folder since your program will read the data directly from the URL. Note: Each student will have a distinct data url for the assignment - so only use your own data url for analysis.\nThe data consists of a number of names and comment counts in JSON as follows:\n{\n  comments: [\n    {\n      name: \"Matthias\"\n      count: 97\n    },\n    {\n      name: \"Geomer\"\n      count: 97\n    }\n    ...\n  ]\n}\nThis is what we want to get:\n$ python3 solution.py\nEnter location: http://py4e-data.dr-chuck.net/comments_42.json\nRetrieving http://py4e-data.dr-chuck.net/comments_42.json\nRetrieved 2733 characters\nCount: 50\nSum: 2...\nCode is found in code/extract_data_json.py\n\nimport urllib.request, urllib.parse, urllib.error\nimport json\nimport ssl \n\n# Ignore SSL certificate errors\nctx = ssl.create_default_context()\nctx.check_hostname = False\nctx.verify_mode = ssl.CERT_NONE\n\n#get data\nurl='http://py4e-data.dr-chuck.net/comments_1701462.json'\ninput = urllib.request.urlopen(url, context=ctx)\ndata = input.read()\n\n#convert\ninfo = json.loads(data)\n\n#extract counts\nnum_list = list()\n\nfor item in info['comments']:\n    num = int(item['count'])\n    num_list.append(num)\n\nprint('Sum:', sum(num_list))\n\nSum: 2300"
  },
  {
    "objectID": "code/6_JSON_and_REST.html#exercise-2-using-the-geojson-api",
    "href": "code/6_JSON_and_REST.html#exercise-2-using-the-geojson-api",
    "title": "6  JSON and REST",
    "section": "6.5 Exercise 2: Using the GeoJSON API",
    "text": "6.5 Exercise 2: Using the GeoJSON API\nThe program will prompt for a location, contact a web service and retrieve JSON for the web service and parse that data, and retrieve the first place_id from the JSON. A place ID is a textual identifier that uniquely identifies a place as within Google Maps.\nTo complete this assignment, you should use this API endpoint that has a static subset of the Google Data:\nhttp://py4e-data.dr-chuck.net/json?\nThis API uses the same parameter (address) as the Google API. This API also has no rate limit so you can test as often as you like. If you visit the URL with no parameters, you get “No address…” response.\nTo call the API, you need to include a key= parameter and provide the address that you are requesting as the address= parameter that is properly URL encoded using the urllib.parse.urlencode() function.\nYou can test to see if your program is working with a location of “South Federal University” which will have a place_id of “ChIJNeHD4p-540AR2Q0_ZjwmKJ8”.\n$ python3 solution.py\nEnter location: South Federal University\nRetrieving http://...\nRetrieved 2445 characters\nPlace id ChIJNeHD4p-540AR2Q0_ZjwmKJ8\nPlease run your program to find the place_id for this location: Marietta College.\nCode is in code/geojson_ex.py\n\nimport urllib.request, urllib.parse, urllib.error\nimport json\nimport ssl\n\napi_key = False\n# If you have a Google Places API key, enter it here\n# api_key = 'AIzaSy___IDByT70'\n# https://developers.google.com/maps/documentation/geocoding/intro\n\nif api_key is False:\n    api_key = 42\n    serviceurl = 'http://py4e-data.dr-chuck.net/json?'\nelse :\n    serviceurl = 'https://maps.googleapis.com/maps/api/geocode/json?'\n\n# Ignore SSL certificate errors\nctx = ssl.create_default_context()\nctx.check_hostname = False\nctx.verify_mode = ssl.CERT_NONE\n\nwhile True:\n    address = input('Enter location: ')\n    if len(address) < 1: break\n\n    parms = dict()\n    parms['address'] = address\n    if api_key is not False: parms['key'] = api_key\n    url = serviceurl + urllib.parse.urlencode(parms)\n\n    print('Retrieving', url)\n    uh = urllib.request.urlopen(url, context=ctx)\n    data = uh.read().decode()\n    print('Retrieved', len(data), 'characters')\n\n    try:\n        js = json.loads(data)\n    except:\n        js = None\n\n    if not js or 'status' not in js or js['status'] != 'OK':\n        print('==== Failure To Retrieve ====')\n        print(data)\n        continue\n\n    print(json.dumps(js, indent=4))\n\n    lat = js['results'][0]['geometry']['location']['lat']\n    lng = js['results'][0]['geometry']['location']['lng']\n    print('lat', lat, 'lng', lng)\n    location = js['results'][0]['formatted_address']\n    print(location)"
  }
]