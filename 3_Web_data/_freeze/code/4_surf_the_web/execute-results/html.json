{
  "hash": "4ff639ce98a90730d50276451a1e22d6",
  "result": {
    "markdown": "# Programs that surf the web\n\n## Unicode characters and strings\n\n- ASCII, American Standard Code for Information Interchange, commonly used mapping of numbers to letters in programming\n- I.e. H == 72\n\n### Representing simple strings\n\n- Each character is represented by a number between 0 and 256 stored in 8 bits of memory\n- We only have a set amount of numbers that can represent characters, so in the earlier days of programming only upper case characters were represented and lower case \"did not exist\"\n- We refer to \"8 bits of memories\" as a **byte**\n- In the 1960s and 1970s we could \"put one character in a byte\"\n- The `ord()` function tells us the numeric value of a simple ASCII character\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nprint(ord('G'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n71\n```\n:::\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nprint(ord('e'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n101\n```\n:::\n:::\n\n\nWith the 2 examples above, we can see that H has a lower ordinal number than e, so H < e. All uppercase are less than lower case letters.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nprint(ord('\\n'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n10\n```\n:::\n:::\n\n\n### Unicode\n\nUnicode is a universal code for hundreds and millions of different characters. \n\n\n### Multi-Byte characters\n\n- To represent the wide range of characters a computer must handle, we represent characters with more than one byte:\n\n- UTF-16: Fixed length, two bytes\n- UTF-32: Fixed length, four bytes. Here, each character uses 4 bytes, so this is less efficient in terms of how much space we need to for example UTF-16\n- **UTF-8**: 1-4 bytes:\n    - Most flexible in terms of space requirements\n    - Upwards compatible with ASCII\n    - Automatic detection between ASCII and UTF-8\n    - UTF-8 is recommended practice for encoding data to be exchanged  between systems\n    \n\n### Two kinds of strings in python\n\n- In Py2 we have two kinds of strings: a \"normal\" str and a unicode str\n- In Py3 all strings are Unicode strings, making all strings the same regardless whether we have Japanese or Latin characters. But in Py3 we have byte strings, i.e. a sequence of bytes that is not human readable and from which we do not know the encoding\n\n\n### Py3 and Unicode\n\n- In Py3 all strings internally are unicode\n- Working with string variables in python programs and reading data usually \"just works\"\n- When we talk to a network resource using sockets or talk to a datase we have to encode and decode data (usually to UTF-8)\n\n\n### Python strings to bytes\n\n- When we talk to an external resource like a network socket we send bytes, so we need to encode the Py3 strings into a given character encoding\n- When we read data from an external resource, we must decode it based on the character set so it is properly represented in Py3 as a string:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport socket \n\nmysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\nmysock.connect(('data.pr4e.org', 80))\n\n#before sending the data we turn them into bytes\ncmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n\n#send request\nmysock.send(cmd)\n\nwhile True:\n    #data we receive is in bytes\n    data = mysock.recv(512)\n    if (len(data) < 1) :\n        break\n    #by default decode() assumes UFT8 or ASCII\n    mystring = data.decide()\n    print(mystring)\n```\n:::\n\n\n## Retrieving webpages\n\nInstead of sockets, we can even go simpler using **urllib**, a library that does all the socket work for us and makes web pages look like a file:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport urllib.request, urllib.parse, urllib.error\n\n#get a handle, that does not contain the data but allows us to access the data\nfhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n\n#iterate through the data using a for loop\nfor line in fhand:\n    print(line.decode().strip())\n\nprint('')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBut soft what light through yonder window breaks\nIt is the east and Juliet is the sun\nArise fair sun and kill the envious moon\nWho is already sick and pale with grief\n\n```\n:::\n:::\n\n\nIn contrast to when we used the socket library we only get the body and not the header with the metadata. But urllib keeps and remembers them, so we could ask for this information.\n\nUrllib kind of handles a webpage like a file and we can use this writing whatever loop we want, i.e. we can count words like this:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nimport urllib.request, urllib.parse, urllib.error\n\n#get a handle, that does not contain the data but allows us to access the data\nfhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n\ncounts = dict()\n\n#iterate through the data using a for loop\nfor line in fhand:\n    words = line.decode().split()\n    for word in words:\n        counts[word] = counts.get(word, 0) + 1\n\nprint(counts)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'But': 1, 'soft': 1, 'what': 1, 'light': 1, 'through': 1, 'yonder': 1, 'window': 1, 'breaks': 1, 'It': 1, 'is': 3, 'the': 3, 'east': 1, 'and': 3, 'Juliet': 1, 'sun': 2, 'Arise': 1, 'fair': 1, 'kill': 1, 'envious': 1, 'moon': 1, 'Who': 1, 'already': 1, 'sick': 1, 'pale': 1, 'with': 1, 'grief': 1}\n```\n:::\n:::\n\n\nThe only difference to handling a \"normal\" file is that we need to remember to decode the data after we retrieved it.\n\nWe can also use this to retrieve html sites:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nimport urllib.request, urllib.parse, urllib.error\n\n#get a handle, that does not contain the data but allows us to access the data\nfhand = urllib.request.urlopen('http://dr-chuck.com/page1.htm')\n\n#iterate through the data using a for loop\nfor line in fhand:\n    print(line.decode().strip())\n\nprint('')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<h1>The First Page</h1>\n<p>\nIf you like, you can switch to the\n<a href=\"http://www.dr-chuck.com/page2.htm\">\nSecond Page</a>.\n</p>\n\n```\n:::\n:::\n\n\n### Following links\n\nIf we want to identify and print all links on a web page,   \ni.e. the `<a href=\"http://www.dr-chuck.com/page2.htm\">\nwe could do it as follows:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nimport urllib.request, urllib.parse, urllib.error\nimport re\n\n#get a handle, that does not contain the data but allows us to access the data\nfhand = urllib.request.urlopen('http://dr-chuck.com/page1.htm')\n\nlink_l = list()\n\n#iterate through the data using a for loop\nfor line in fhand:\n    line = line.decode().strip()\n    link = re.findall('href=\"(http:.+)\"', line)\n    if len(link) == 0:\n        continue\n    link_l = link_l + link\n\nprint(link_l)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['http://www.dr-chuck.com/page2.htm']\n```\n:::\n:::\n\n\n## What is web scraping?\n\n- When a program or a script pretends to be a browser and retrieves web pages, looks at those web pages, extracts information, and then looks at more web pages\n- Search engines scrape web pages, we call this \"spidering the web\" or \"web crawling\"\n\n\n## Why scrape?\n\n- To pull out data, particularly social data, who links who?\n- Get your own data back out of some system that has no export capability\n- Monitor a site for new information\n- Spider the web to make a database or a search engine\n- Keep in mind, you have to be allowed to access the data\n- There is some controversy about scraping and some sites have some strict rules around it\n- Republishing copyrighted information is not allowed\n- Violating terms of service is not allowed\n\n\n## Beautiful Soup for parsing HTML\n\n- You could do HTML searches the hard way (since HTML is not allways super consistent)\n- Or use a free software library called BeautifulSoup \n- To run this software, you have to first install it\n\nCode is found in `code/urllinks.py`\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nimport urllib.request, urllib.parse, urllib.error\nfrom bs4 import BeautifulSoup\n\nurl = 'http://www.dr-chuck.com/page1.htm'\nhtml = urllib.request.urlopen(url).read()\n\n#beautify the html object\nsoup = BeautifulSoup(html, 'html.parser')\n\n#retrieve all of the anchor tags\n#this code below returns a list\ntags = soup('a')\n\n#extract the href key or nothing\nfor tag in tags:\n    print(tag.get('href', None))\n\nprint('')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nhttp://www.dr-chuck.com/page2.htm\n\n```\n:::\n:::\n\n\nIf we have pages that give us SSL certificate errors, we can adjust the code like this:\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nimport urllib.request, urllib.parse, urllib.error\nfrom bs4 import BeautifulSoup\nimport ssl\n\n# Ignore SSL certificate errors\nctx = ssl.create_default_context()\nctx.check_hostname = False\nctx.verify_mode = ssl.CERT_NONE\n\n#beautify the html object\nurl = 'http://www.dr-chuck.com/page1.htm'\nhtml = urllib.request.urlopen(url, context=ctx).read()\nsoup = BeautifulSoup(html, 'html.parser')\n\n#retrieve all of the anchor tags\n#this code below returns a list\ntags = soup('a')\n\n#extract the href key or nothing\nfor tag in tags:\n    print(tag.get('href', None))\n\nprint('')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nhttp://www.dr-chuck.com/page2.htm\n\n```\n:::\n:::\n\n\n## Exercise \n\n The program will use urllib to read the HTML from the data files below, and parse the data, extracting numbers and compute the sum of the numbers in the file.\n\nWe provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n\n- Sample data: http://py4e-data.dr-chuck.net/comments_42.html (Sum=2553)\n- Actual data: http://py4e-data.dr-chuck.net/comments_1701459.html (Sum ends with 85)\n\nThe file is a table of names and comment counts. You can ignore most of the data in the file except for lines like the following:\n\n```\n<tr><td>Modu</td><td><span class=\"comments\">90</span></td></tr>\n<tr><td>Kenzie</td><td><span class=\"comments\">88</span></td></tr>\n<tr><td>Hubert</td><td><span class=\"comments\">87</span></td></tr>\n```\n\nYou are to find all the <span> tags in the file and pull out the numbers from the tag and sum the numbers.\n\nCode is found in `code/exercise.py`\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nimport urllib.request, urllib.parse, urllib.error\nfrom bs4 import BeautifulSoup\nimport re\n\n#url = \nurl = 'http://py4e-data.dr-chuck.net/comments_1701459.html'\nhtml = urllib.request.urlopen(url).read()\n\n#beautify\nsoup = BeautifulSoup(html, 'html.parser')\n\n#retrieve span tags\ntags = soup('span')\n\n#pull out nrs\nnumlist = list()\n\nfor tag in tags:\n    tag_string = int(tag.contents[0])\n    numlist.append(tag_string)\n\nprint('Count', len(numlist))\nprint('Sum', sum(numlist))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCount 50\nSum 2685\n```\n:::\n:::\n\n\n## Exercise 2\n\nThe program will use urllib to read the HTML from the data files below, extract the href= vaues from the anchor tags, scan for a tag that is in a particular position relative to the first name in the list, follow that link and repeat the process a number of times and report the last name you find.  \n\nWe provide two files for this assignment. One is a sample file where we give you the name for your testing and the other is the actual data you need to process for the assignment\n\nSample problem: Start at http://py4e-data.dr-chuck.net/known_by_Fikret.html\nFind the link at position 3 (the first name is 1). Follow that link. Repeat this process 4 times. The answer is the last name that you retrieve.\nSequence of names: Fikret Montgomery Mhairade Butchi Anayah\nLast name in sequence: Anayah\n  \nActual problem: Start at: http://py4e-data.dr-chuck.net/known_by_Josh.html\nFind the link at position 18 (the first name is 1). Follow that link. Repeat this process 7 times. The answer is the last name that you retrieve.\nHint: The first character of the name of the last page that you will load is: G\n\n**Strategy**\nThe web pages tweak the height between the links and hide the page after a few seconds to make it difficult for you to do the assignment without writing a Python program. But frankly with a little effort and patience you can overcome these attempts to make it a little harder to complete the assignment without writing a Python program. But that is not the point. The point is to write a clever Python program to solve the program.\n\n**Sample execution**\nHere is a sample execution of a solution:\n\n```\n$ python3 solution.py\nEnter URL: http://py4e-data.dr-chuck.net/known_by_Fikret.html\nEnter count: 4\nEnter position: 3\nRetrieving: http://py4e-data.dr-chuck.net/known_by_Fikret.html\nRetrieving: http://py4e-data.dr-chuck.net/known_by_Montgomery.html\nRetrieving: http://py4e-data.dr-chuck.net/known_by_Mhairade.html\nRetrieving: http://py4e-data.dr-chuck.net/known_by_Butchi.html\nRetrieving: http://py4e-data.dr-chuck.net/known_by_Anayah.html\n```\n\n",
    "supporting": [
      "4_surf_the_web_files"
    ],
    "filters": [],
    "includes": {}
  }
}