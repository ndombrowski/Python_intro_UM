# Building a Search Engine - Introduction

This week we will download and run a simple version of the Google PageRank Algorithm.  Here is an early paper by Larry Page and Sergy Brin, the founders of Google, that describes their early thoughts about the algorithm:

http://infolab.stanford.edu/~backrub/google.html

We will provide you with sample code and lectures that walk through the sample code:

https://www.py4e.com/code3/pagerank.zip

There is not a lot of new code to write - it is mostly looking at the code and making the code work.  You will be able to spider some simple content that we provide and then play with the program to spider some other content.  Part of the fun of this assignment is when things go wrong and you figure out how to solve a problem when the program wanders into some data that breaks its retrieval and parsing.  So you will get used to starting over with a fresh database and running your web crawl.

## Web crawler

A web crawler is a computer program that browses the Web in a methodological, automated manner. Web crawlers are mainly used to create a copy of all the visited pages for later processing by a search engine that will index the downloaded pages to provide fast searches.  

It generally will:

- Retrieve a page
- Look through the page for links
- Add the links to a list of "to be retrieved" sites
- Repeat


## Web crawling policies

- A selection policy states which pages to download
- A revisit policy states when to check for changes to the pages
- A politeness policy states how to avoid overloading web sites
- A parallelization policy states how to coordinate distributed web crawlers


## robots.txt

- A way for a website to communicate with web crawlers
- An informal and voluntary standard
- Sometimes folks make a "spider trap" to catch "bad spiders"
- See an example in `data/robots.txt`


## Search indexing

Search engine indexing collects, parses and stores data to facilitate fast and accurate information retrieval. The purpose of storing an index is to optimize speed and performance in finding relevant documents for a search query. Without an index, the search engine would scan every document in the corpus, which would require considerable time and computing power.

<img width=500 src="../images/workflow1.png">


## Worked Example: Simple Python Search Spider, Page Ranker, and Visualizer

This is a set of programs that emulate some of the functions of a 
search engine. They store their data in a SQLITE3 database named
'spider.sqlite'. This file can be removed at any time to restart the
process.   

This program crawls a web site and pulls a series of pages into the
database, recording the links between pages.

```
python3 spider.py
Enter web url or enter: http://www.dr-chuck.com/
['http://www.dr-chuck.com']
How many pages:2
```

https://www.embl.org

In this sample run, we told it to crawl a website and retrieve two 
pages. If you restart the program again and tell it to crawl more
pages, it will not re-crawl any pages already in the database. Upon 
restart it goes to a random non-crawled page and starts there. So 
each successive run of spider.py is additive.

You can have multiple starting points in the same database - 
within the program, these are called "webs". The spider
chooses randomly amongst all non-visited links across all
the webs.

If you want to dump the contents of the spider.sqlite file, you can 
run spdump.py as follows:

``` python3 spdump.py ```

```
(5, None, 1.0, 3, u'http://www.dr-chuck.com/csev-blog')
(3, None, 1.0, 4, u'http://www.dr-chuck.com/dr-chuck/resume/speaking.htm')
(1, None, 1.0, 2, u'http://www.dr-chuck.com/csev-blog/')
(1, None, 1.0, 5, u'http://www.dr-chuck.com/dr-chuck/resume/index.htm')
4 rows.
```

This shows the number of incoming links, the old page rank, the new page rank, the id of the page, and the url of the page. The spdump.py program only shows pages that have at least one incoming link to them.

Once you have a few pages in the database, you can run Page Rank on the
pages using the sprank.py program. You simply tell it how many Page
Rank iterations to run.

```
python3 sprank.py
```

You can dump the database again to see that page rank has been updated.

If you want to visualize the current top pages in terms of page rank,
run spjson.py to write the pages out in JSON format to be viewed in a
web browser.

```
python3 spjson.py
```

You can view this data by opening the file force.html in your web browser. This shows an automatic layout of the nodes and links. You can click and drag any node and you can also double click on a node to find the URL
that is represented by the node.

This visualization is provided using the force layout from:

http://mbostock.github.com/d3/






